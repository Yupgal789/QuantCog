{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 3: Data as a Random Variable\n",
    "## Intro to Quantified Cognition\n",
    "By: Per B. Sederberg, PhD\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/2020_Spring/notebooks/03_Data_As_A_Random_Variable.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson Plan\n",
    "\n",
    "- Introduce data as a random variable\n",
    "- Probability distributions as models of the world\n",
    "- Statistics as performing inference on those models\n",
    "- Assessing model fit\n",
    "- Model comparison\n",
    "- A real(ish) example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data as a random variables\n",
    "\n",
    "Most statistics operates under the assumption that any observed data are actual samples drawn from some (to be learned) model of the world.\n",
    "\n",
    "In the standard approach, these models are simply probability distributions, with parameters that govern the behavior of the model (i.e., what we can expect they will produce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uncertainty\n",
    "\n",
    "If the observed data samples are simply random draws from a probability distribution, then the level of uncertainty will decrease as we gain more data samples.\n",
    "\n",
    "Statistical inference involves figuring out what model (e.g., probability distribution, but we will be building more complicated models later in the course) and parameters generated the data. \n",
    "\n",
    "Let's spend some time trying to perform this task by hand..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *ONLY* if on Google Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the dists.py and data files\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2020_Spring/notebooks/dists.py\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2020_Spring/notebooks/random_data.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "import ipywidgets as widgets      # interactive widgets\n",
    "from ipywidgets.widgets.interaction import show_inline_matplotlib_plots\n",
    "from IPython.display import display, clear_output\n",
    "import pickle\n",
    "\n",
    "# import the distributions wrapped from scipy\n",
    "import dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "with open('random_data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# tell us something about the data\n",
    "for i,d in enumerate(data):\n",
    "    print('Dataset %d has %d samples' % (i, len(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# mapper between dist name and object\n",
    "dist_dict = {'Beta': dists.beta,\n",
    "             'Exponential': dists.exp,\n",
    "             'Gamma': dists.gamma,\n",
    "             'Normal': dists.normal,\n",
    "             'Uniform': dists.uniform}\n",
    "\n",
    "# Dropdown widget to pick datasets\n",
    "ds_ind = widgets.Dropdown(options=range(len(data)),\n",
    "                          description=\"Dataset\")\n",
    "\n",
    "# Checkbox for whether to show log likelihood\n",
    "like_check = widgets.Checkbox(description='Show Log Likelihood', \n",
    "                              value=False)\n",
    "\n",
    "# Checkbox for whether to show stem plot\n",
    "stem_check = widgets.Checkbox(description='Show Stem Plot', \n",
    "                              value=False)\n",
    "\n",
    "# set up the parameters\n",
    "children = []\n",
    "\n",
    "# beta params\n",
    "beta_button = widgets.Button(description='Beta', icon='check')\n",
    "beta_alpha = widgets.FloatText(value=.5,\n",
    "                               description='alpha')\n",
    "beta_beta = widgets.FloatText(value=.5,\n",
    "                              description='beta')\n",
    "children.append(widgets.HBox([beta_button, beta_alpha, beta_beta]))\n",
    "\n",
    "# exp params\n",
    "exp_button = widgets.Button(description='Exponential', icon='')\n",
    "exp_lam = widgets.FloatText(value=5.0,\n",
    "                            description='lam')\n",
    "children.append(widgets.HBox([exp_button, exp_lam]))\n",
    "\n",
    "# gamma params\n",
    "gamma_button = widgets.Button(description='Gamma', icon='')\n",
    "gamma_alpha = widgets.FloatText(value=.5,\n",
    "                                description='alpha')\n",
    "gamma_beta = widgets.FloatText(value=.5,\n",
    "                              description='beta')\n",
    "children.append(widgets.HBox([gamma_button, gamma_alpha, gamma_beta]))\n",
    "\n",
    "# normal params\n",
    "normal_button = widgets.Button(description='Normal', icon='')\n",
    "normal_mean = widgets.FloatText(value=0.0,\n",
    "                                description='mean')\n",
    "normal_std = widgets.FloatText(value=1.0,\n",
    "                               description='std')\n",
    "children.append(widgets.HBox([normal_button, normal_mean, normal_std]))\n",
    "\n",
    "# uniform params\n",
    "uniform_button = widgets.Button(description='Uniform', icon='')\n",
    "uniform_lower = widgets.FloatText(value=0.0,\n",
    "                                  description='lower')\n",
    "uniform_upper = widgets.FloatText(value=1.0,\n",
    "                                  description='upper')\n",
    "children.append(widgets.HBox([uniform_button, uniform_lower, uniform_upper]))\n",
    "\n",
    "\n",
    "# set the full user interface\n",
    "ui = widgets.VBox([widgets.HBox([ds_ind, like_check, stem_check])]+children)\n",
    "\n",
    "\n",
    "# define plotting function\n",
    "def plot_data_and_dist(*vals, **kwargs):\n",
    "    # first plot the data\n",
    "    dat = data[ds_ind.value]\n",
    "    plt.hist(dat, bins='auto', density=True, alpha=.5);\n",
    "    \n",
    "    # now plot the pdf of the dist\n",
    "    npoints = 100\n",
    "    \n",
    "    # add support for 10% of the data range on either side\n",
    "    support = (dat.min() - np.ptp(dat)*.1,\n",
    "               dat.max() + np.ptp(dat)*.1)\n",
    "    x = np.linspace(support[0], support[1], npoints)\n",
    "    \n",
    "    # get the selected dist and params\n",
    "    selected_index = [c.children[0].icon=='check' for c in children].index(True)\n",
    "    params = {c.description: c.value for c in children[selected_index].children[1:]}\n",
    "    #params = {c.description: c.value for c in \n",
    "    #          dist_tab.children[dist_tab.selected_index].children}\n",
    "    dist = dist_dict[children[selected_index].children[0].description](**params)\n",
    "    \n",
    "    # calculate the pdf\n",
    "    pdf = dist.pdf(x)\n",
    "            \n",
    "    # plot the pdf and add labels\n",
    "    plt.plot(x, pdf, lw=3)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')\n",
    "    if like_check.value:\n",
    "        # calculate the log like\n",
    "        log_like = np.log(dist.pdf(dat)).sum()\n",
    "\n",
    "        # add it to the plot with some formatting\n",
    "        plt.title('Log Like: {:3.4f}'.format(log_like))\n",
    "    \n",
    "    if stem_check.value:\n",
    "        # include the stem plot\n",
    "        plt.stem(dat, dist.pdf(dat), 'g')\n",
    "\n",
    "        \n",
    "# set up triggers for updating the plot\n",
    "out = widgets.interactive_output(plot_data_and_dist, \n",
    "                                 {'ds_ind': ds_ind,\n",
    "                                  'like_check': like_check,\n",
    "                                  'stem_check': stem_check,\n",
    "                                  'beta_alpha': beta_alpha,\n",
    "                                  'beta_beta': beta_beta,\n",
    "                                  'exp_lam': exp_lam,\n",
    "                                  'gamma_alpha': gamma_alpha,\n",
    "                                  'gamma_beta': gamma_beta,\n",
    "                                  'normal_mean': normal_mean,\n",
    "                                  'normal_std': normal_std,\n",
    "                                  'uniform_lower': uniform_lower,\n",
    "                                  'uniform_upper': uniform_upper,\n",
    "                                 })\n",
    "\n",
    "\n",
    "# handle button presses\n",
    "def on_button_press(b):\n",
    "    # make sure that button is checked\n",
    "    for c in children:\n",
    "        if c.children[0] == b:\n",
    "            c.children[0].icon = 'check'\n",
    "        else:\n",
    "            c.children[0].icon = ''\n",
    "    \n",
    "    # call the plot function\n",
    "    plot_controls = {'ds_ind': ds_ind,\n",
    "                   'like_check': like_check,\n",
    "                   'stem_check': stem_check,\n",
    "                   'beta_alpha': beta_alpha,\n",
    "                   'beta_beta': beta_beta,\n",
    "                   'exp_lam': exp_lam,\n",
    "                   'gamma_alpha': gamma_alpha,\n",
    "                   'gamma_beta': gamma_beta,\n",
    "                   'normal_mean': normal_mean,\n",
    "                   'normal_std': normal_std,\n",
    "                   'uniform_lower': uniform_lower,\n",
    "                   'uniform_upper': uniform_upper,\n",
    "                  }\n",
    "    kwargs = {k:v.value for k,v in plot_controls.items()}\n",
    "    show_inline_matplotlib_plots()\n",
    "    with out:\n",
    "        clear_output(wait=True)\n",
    "        plot_data_and_dist(**kwargs)\n",
    "        show_inline_matplotlib_plots()\n",
    "for c in children:\n",
    "    c.children[0].on_click(on_button_press)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# show everything\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Likelihood calculation\n",
    "\n",
    "We don't have to do this by eye. Because our model is a probability density function (PDF), we can calculate the likelihood of the data given the model and parameters.\n",
    "\n",
    "For any given model and parameters, you can determine the probability of having observed any individual data point by evaluating the PDF at the value of that data point (*Turn on the Stem Plot.*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "The goal then becomes to maximize the likelihood of observing the data given a model and parameters:\n",
    "\n",
    "$$P(D \\mid \\theta, M)$$\n",
    "\n",
    "As long as the data points are all independent, the likelihood of observing all of them is the product of all the probabilities.\n",
    "\n",
    "It is more efficient and computationally tractable to perform this in log-space, so we typically convert the likelihood into a sum of log likelihoods.\n",
    "\n",
    "*Let's turn on the Log Likelihood checkbox above and see if we can do better!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automated optimization\n",
    "\n",
    "Many approaches have been developed for searching parameter spaces to find the parameters that generate the maximum or minimum value of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nelder--Mead Simplex\n",
    "\n",
    "One very popular algorithm is the Nelder--Mead simplex.\n",
    "\n",
    "It involves growing and shrinking a simplex (a generalization of a triangle to multiple dimensions) to search the parameter space efficiently to minimize a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Nelder-Mead_Himmelblau.gif/640px-Nelder-Mead_Himmelblau.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# scipy includes lots of optimization methods\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def like_fun(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    model = args[0]\n",
    "    dat = args[1]\n",
    "    \n",
    "    # instantiate the model with the params\n",
    "    dist = model(*params)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the best-fitting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the distribution and data from the UI above\n",
    "dist_name = \"Normal\"   # MODIFY THIS (one of the models defined above)\n",
    "ind = 4                 # MODIFY THIS, TOO (numeric index of the data)\n",
    "model = dist_dict[dist_name]\n",
    "dat = data[ind]           \n",
    "\n",
    "# set the bounds for the distribution\n",
    "bound_dict = {'Beta': [(0, 10), (0, 10)],\n",
    "              'Exponential': [(0, 20)],\n",
    "              'Gamma': [(0, 10), (0, 10)],\n",
    "              'Normal': [(-10, 10), (0, 10)],\n",
    "              'Uniform': [(-10, 5), (-5, 10)]}\n",
    "bounds = bound_dict[dist_name]\n",
    "\n",
    "# generate a random starting point based on the bounds\n",
    "# NB: it's possible to generate invalid starting points\n",
    "x0 = [dists.uniform(*b).rvs() for b in bounds]\n",
    "\n",
    "# print some information about the distribution and starting values\n",
    "print('Dataset:', ind)\n",
    "print('Distribution:', dist_name)\n",
    "print('Starting value:', x0)\n",
    "print()\n",
    "\n",
    "# run the optimizer\n",
    "# NOTE, not all methods make use of the bounds method\n",
    "res = opt.minimize(like_fun, x0, args=(model, dat), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualize the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the grid of points to evaluate\n",
    "x = np.linspace(bounds[0][0], bounds[0][1], 100)\n",
    "y = np.linspace(bounds[1][0], bounds[1][1], 100)\n",
    "xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "\n",
    "# evaluate the log likelihood\n",
    "z = np.log(model(xx, yy).pdf(dat[:, np.newaxis, np.newaxis])).sum(0)\n",
    "z[z<-100] = -100\n",
    "\n",
    "# plot the contour and the best-fit value\n",
    "plt.contourf(x, y, z, 100)\n",
    "plt.plot(res.x[0], res.x[1], 'x', markersize=12, color='red')\n",
    "plt.colorbar()\n",
    "#plt.xlabel()\n",
    "#plt.ylabel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Assessing model fit\n",
    "\n",
    "We could simply compare the maximum likelihoods between the models, but that doesn't take into account the complexity of each model and amount of data. \n",
    "\n",
    "One, more principled, approach is Bayesian Information Criterion (BIC):\n",
    "\n",
    "$$BIC = \\text{ln}(n)k - 2\\text{ln}(\\hat{L}),$$\n",
    "\n",
    "where $n$ is the number of data points, $k$ is the number of parameters, and $\\hat{L}$ is the maximum likelihood value of the model $M$:\n",
    "\n",
    "$$\\hat{L} = P(x \\mid \\hat{\\theta}, M),$$\n",
    "\n",
    "where $\\hat{\\theta}$ are the parameters that maximize the model's likelihood function and $x$ are the observed data.\n",
    "\n",
    "***SMALLER BIC values are better!!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$BIC = \\text{ln}(n)k - 2\\text{ln}(\\hat{L}),$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model fit with BIC\n",
    "# recall that the optimization returns the negative log likelihood\n",
    "n = len(dat)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "But how do we decide if one model is better than the other? We can compare BIC values between models, turning them into a Bayes Factor!\n",
    "\n",
    "$$BF_{01} = exp((BIC_0 - BIC_1)/2)$$\n",
    "\n",
    "This is interpreted with the help of the following guidelines:\n",
    "\n",
    "| Bayes Factor | Evidence |\n",
    "|--------------|----------|\n",
    "| 1--3         | Weak     |\n",
    "| 3--20        | Positive |\n",
    "| 20--150      | Strong   |\n",
    "| >150         | Very Strong | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc Bayes Factor\n",
    "# (enter numbers from the BIC assessments above)\n",
    "bic_0 = 117.1368\n",
    "bic_1 = 111.11482\n",
    "np.exp((bic_0 - bic_1)/2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## t-test example\n",
    "\n",
    "We now have all the tools necessary to perform statistical inference (though we will improve on all these approaches in the coming weeks). You can:\n",
    "\n",
    "- Use optimization techniques to identify the parameters that give rise to the maximum likelihood of observing the data given the model\n",
    "- Assess model fit\n",
    "- Compare models to guide model selection\n",
    "\n",
    "Let's try a simple example of performing a t-test via model comparison approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# function to help plot a PDF\n",
    "def plot_pdf(dist, support=[-5, 5], npoints=100):\n",
    "    # set a range of linearly-spaced points\n",
    "    x = np.linspace(support[0], support[1], npoints)\n",
    "    \n",
    "    # evaluate the pdf at those points\n",
    "    pdf = dist.pdf(x)\n",
    "    \n",
    "    # plot the results\n",
    "    plt.plot(x, pdf, lw=3)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the student's T distribution\n",
    "plot_pdf(dists.normal(mean=0.0, std=1.0), support=[-5, 5])\n",
    "plot_pdf(dists.students_t(mean=0.0, std=1.0, df=1.0), support=[-5, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data that may or may not be significantly different from zero\n",
    "A = dists.normal(.3, .5).rvs(10)\n",
    "\n",
    "# plot it\n",
    "plt.hist(A, bins='auto', density=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard t-test\n",
    "\n",
    "First we'll perform a standard one-sample t-test on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a one-sample t-test\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.ttest_1samp(A, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a Student's t model\n",
    "\n",
    "Next we fit the full model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def students_like(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    dat = args[0]\n",
    "    df = len(dat) - 1\n",
    "    \n",
    "    # instantiate the model with the params, \n",
    "    # the df is determined from the data\n",
    "    dist = dists.students_t(params[0], params[1], df)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like\n",
    "\n",
    "# pick an central starting point\n",
    "x0 = [0.0, 1.0]\n",
    "\n",
    "# run the optimization\n",
    "res = opt.minimize(students_like, x0, args=(A,), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)\n",
    "\n",
    "# calculate the BIC for this model and save it\n",
    "n = len(A)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic_1 = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fit a null hypothesis model\n",
    "\n",
    "Now we fit a model representing the null hypothesis that the mean of the data is actuall 0.0. \n",
    "\n",
    "Note how we simply fix the mean of the Student's t distribution to zero, but still fit the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a likelihood function\n",
    "def students_null_like(params, *args):\n",
    "    # pull the model and dat out of the args\n",
    "    dat = args[0]\n",
    "    df = len(dat) - 1\n",
    "    \n",
    "    # instantiate the model with the params\n",
    "    # mean is fixed at zero and the df is determined from the data\n",
    "    dist = dists.students_t(0.0, params[0], df)\n",
    "    \n",
    "    # calc the log like\n",
    "    log_like = np.log(dist.pdf(dat)).sum()\n",
    "    if np.isnan(log_like):\n",
    "        log_like = -np.inf\n",
    "    \n",
    "    # return the negative of it to minimize\n",
    "    return -log_like\n",
    "\n",
    "# start at same point (though mean is fixed at zero)\n",
    "x0 = [1.0]\n",
    "\n",
    "# run the optimization\n",
    "res = opt.minimize(students_null_like, x0, args=(A,), \n",
    "                   #bounds=bounds,\n",
    "                   #method='L-BFGS-B',\n",
    "                   #method='BFGS',\n",
    "                   method='Nelder-Mead',\n",
    "                   #method='TNC'\n",
    "                  )\n",
    "print(res)\n",
    "\n",
    "# calculate and print the BIC\n",
    "n = len(A)\n",
    "k = len(res.x)\n",
    "L = -res.fun\n",
    "bic_0 = np.log(n)*k - 2*(L)\n",
    "print('BIC:', bic_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Comparison\n",
    "\n",
    "Now that we have the BIC values for each model, we can use the Bayes Factor to determine whether the full model is preferred to the null model. \n",
    "\n",
    "We want a big number here. If it's less than 1.0 then there is no evidence that the alternative/full model should be preferred to the null model (i.e., the mean of the distribution is not different from 0.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Bayes Factor\n",
    "bf = np.exp((bic_0 - bic_1)/2.)\n",
    "print('Bayes Factor:', bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next class\n",
    "\n",
    "This approach still does not properly take into account uncertainty in the assessment of the model fit, which can give rise to spurious results due to lucky sampling.\n",
    "\n",
    "Next we'll start to learn about Bayesian approaches to model fitting and comparison, which can provide a more principled way to perform statistical inference."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
