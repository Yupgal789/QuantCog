{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 12: Final Project\n",
    "\n",
    "## Intro to Quantified Cognition\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/master/notebooks/12_Final_Project.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Project\n",
    "\n",
    "- The goal of the final project is to demonstrate some application of some of the modeling tools we've discussed this semester. \n",
    "\n",
    "- Unless you are developing a novel theory, most mechanistic cognitive modeling involves making use of an existing model, which you then may tweak for your needs/hypotheses.\n",
    "\n",
    "- Below I list some options for this final project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Info\n",
    "\n",
    "- Please turn in the project in the form of a Jupyter notebook, along with any other files I might need to run the notebook.\n",
    "\n",
    "- It is due ***Wednesday, May 8th, 2019***.\n",
    "\n",
    "- I will be available for meetings from now until it is due. Please email to schedule one.\n",
    "\n",
    "- Feel free to work together in teams of *up to a maximum of 3 people*, but make it *VERY CLEAR* on your Jupyter notebook submission who was part of the team so you can all get credit.\n",
    "\n",
    "- Please make use of other members of the class for help along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option 0: Some analysis of your own data\n",
    "\n",
    "Many of you have datasets from your own research. Your project would entail performing some form of Bayesian or cognitive model-based analysis of those data. \n",
    "\n",
    "Note, it would be acceptable to use PyMC3 and build a Bayesian model to generate your data, as long as we have not performed the specific analysis already in class.\n",
    "\n",
    "You must include the following:\n",
    "\n",
    "- A short intro (a couple sentences) explaining the question you'll be asking (to frame the analysis)\n",
    "- Test some alternative model variants\n",
    "- Some form of model comparison (via Bayes Factor, BPIC, WAIC, etc...)\n",
    "- Show model fit (at least best-fitting params, but potentially posterior predictives)\n",
    "- Summary of findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option 1: Generating Model Contest\n",
    "\n",
    "Very often we have some data and our goal is to identify what model might have generated those data. For this project, I have generated sets of data with two different decision models: WFPT and LBA. \n",
    "\n",
    "Your job is to fit models (and model varariants with and without some params free, such as between-trial variability in drift rate) to identify what models (and parameter values, as best fits or posteriors) were used to generate each set of data.\n",
    "\n",
    "This will require model comparison, via some method of your choosing (Bayes Factor, BPIC, WAIC, etc...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: LBA Model Extension\n",
    "\n",
    "Extend LBA to include generation of a confidence value, in addition to just a choice and reaction time. One method of achieving this (though I'm open to other approaches as long as you justify them) is to assume that confidence is directly proportional to the level of activation for the accumulator with the winning choice relative to the sum of all the accumulator activations at that time. \n",
    "\n",
    "Intuitively, this approach makes some sense. If the selected choice has a high level of activation relative to the non-selected choice, then the confidence will be high (close to 1.0). On the other hand, if there is strong evidence for both choices and one just barely wins out over the other, then the ratio of winning to all choices will be closer to .5.\n",
    "\n",
    "To test whether this model is, indeed, making predictions that make sense, pick the variant of the LBA model that fit best to the speed--accuracy trade-off decision data (the one that allowed the drift rates to change between conditions) and perform the fit again with this new model. Even though you are not fitting to confidence, we can simulate the model with the best-fitting parameters and generate a distribution of confidence values for the speed condition and confidence values for the accuracy condition. Some questions to answer:\n",
    "\n",
    "- Are people more confident in the accuracy condition? \n",
    "- Are the confidence values different for correct and incorrect answers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Flanker Analysis\n",
    "\n",
    "While there have been full models of cognitive control proposed to account for the dynamics of the decision process in congruent vs. incongruent conditions, insight can be gained by fitting to these two conditions separately with a standard decision-making model.\n",
    "\n",
    "- Fit the WFPT model separately to the incongruent and congruent trials from the flanker task\n",
    "- Decide what parameters should be kept constant between the two conditions and what parameters should be allowed to change\n",
    "- Justify this decision (perhaps even via a model comparison)\n",
    "- Show fits of the best-fitting parameters (or posterior predictives) to the data\n",
    "- Show full posteriors for the parameters and discuss whether they make sense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *ONLY* if on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install RunDEMC\n",
    "!pip install git+https://github.com/compmem/RunDEMC.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the data\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/decision_data.csv\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/contest_dataset_1.csv\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/contest_dataset_2.csv\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/contest_dataset_3.csv\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/flanker_s1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to retrieve the wfpt model\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/master/notebooks/wfpt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading scoop, reverting to joblib.\n"
     ]
    }
   ],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from scipy import stats\n",
    "\n",
    "from RunDEMC.density import kdensity\n",
    "from RunDEMC import Model, Param, dists, calc_bpic, joint_plot\n",
    "\n",
    "from wfpt import wfpt_like, wfpt_gen\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "try:\n",
    "    import scoop\n",
    "    from scoop import futures\n",
    "except ImportError:\n",
    "    print(\"Error loading scoop, reverting to joblib.\")\n",
    "    scoop = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def lba_sim(I=(1.0,1.5), A=.1, S=1.0, b=1.0, t0=0.0, \n",
    "            num_sims=1000, max_time=2., I_scales_S=False, **kwargs):\n",
    "    # set drift rate from inputs\n",
    "    dr = np.float64(I)\n",
    "    \n",
    "    # set the number of choices\n",
    "    nc = len(dr)\n",
    "    \n",
    "    # pick starting points\n",
    "    k = np.random.uniform(0., A, (num_sims, nc))\n",
    "    \n",
    "    # pick drifts\n",
    "    if I_scales_S:\n",
    "        # calc S from drift rates\n",
    "        S = np.sqrt((dr**2).sum())*S\n",
    "        \n",
    "    # must make sure at least one d is greater than zero for each sim\n",
    "    d = np.random.normal(dr, S, (num_sims, nc))\n",
    "    \n",
    "    # see where there are none above zero\n",
    "    #ind = np.all(d<=0.0,axis=1)\n",
    "    #while np.any(ind):\n",
    "    #    d[ind,:] = np.random.normal(dr,S,(ind.sum(),nc))\n",
    "    #    ind = np.all(d<=0.0,axis=1)\n",
    "\n",
    "    # clip it to avoid divide by zeros\n",
    "    d[d<=0.0] = np.finfo(dr.dtype).eps\n",
    "\n",
    "    # calc the times for each\n",
    "    t = (b-k)/d\n",
    "\n",
    "    # see the earliest for each resp\n",
    "    inds = t.argmin(1)\n",
    "    times = t.take(inds+np.arange(t.shape[0])*t.shape[1])\n",
    "\n",
    "    # process into choices\n",
    "    times += t0\n",
    "    \n",
    "    # get valid responses\n",
    "    resp_ind = times < (max_time)\n",
    "    resp = inds+1\n",
    "    resp[~resp_ind] = 0\n",
    "    \n",
    "    # return as data frame\n",
    "    return pd.DataFrame.from_dict({'choice':resp, 'rt':times})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>rt</th>\n",
       "      <th>cond</th>\n",
       "      <th>log_rt</th>\n",
       "      <th>rt_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4784</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.737308</td>\n",
       "      <td>0.4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.843970</td>\n",
       "      <td>0.4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.801624</td>\n",
       "      <td>0.4486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.3991</td>\n",
       "      <td>Speed</td>\n",
       "      <td>-0.918543</td>\n",
       "      <td>0.3991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>-0.822573</td>\n",
       "      <td>0.4393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   correct      rt      cond    log_rt  rt_acc\n",
       "3        1  0.4784  Accuracy -0.737308  0.4784\n",
       "4        1  0.4300  Accuracy -0.843970  0.4300\n",
       "5        1  0.4486  Accuracy -0.801624  0.4486\n",
       "6        1  0.3991     Speed -0.918543  0.3991\n",
       "8        1  0.4393  Accuracy -0.822573  0.4393"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load in the data\n",
    "dat = pd.read_csv('decision_data.csv', index_col=0)\n",
    "dat = dat[dat.cond != 'Neutral']\n",
    "dat['rt_acc'] = dat['rt']\n",
    "dat.loc[dat.correct==0,'rt_acc'] = -dat['rt']\n",
    "dat.head()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
