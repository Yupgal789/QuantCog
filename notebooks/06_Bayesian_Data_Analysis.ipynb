{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 6: Bayesian Data Analysis\n",
    "\n",
    "## Intro to Quantified Cognition\n",
    "\n",
    "By: Per B. Sederberg, PhD\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/2020_Spring/notebooks/06_Bayesian_Data_Analysis.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson plan\n",
    "\n",
    "- Hierarchical models\n",
    "- Deep dive into various Bayesian models of one dataset\n",
    "- Explore other datasets\n",
    "- Real-world data analysis in teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if on Google Colab\n",
    "#!pip install git+https://github.com/arviz-devs/arviz\n",
    "!pip install arvis\n",
    "\n",
    "# to retrieve the dists.py and data files\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2020_Spring/notebooks/dists.py\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2020_Spring/notebooks/flanker_dat.csv\n",
    "!wget https://raw.githubusercontent.com/compmem/QuantCog/2020_Spring/notebooks/rdm_dataframe.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# Turn off future warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import arviz as az\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Regression\n",
    "\n",
    "What should we do when we have multiple groups/subjects?\n",
    "\n",
    "Ideally we should share information across groups to inform the models fit to the individuals.\n",
    "\n",
    "This is called multi-level or hierarchical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flanker dataset\n",
    "\n",
    "We will explore a number of ways to analyze a flanker task dataset. \n",
    "\n",
    "You can then use these as a guide when you analyze some of the other datasets we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "dat = pd.read_csv('flanker_dat.csv', index_col=None)\n",
    "\n",
    "# remove nans\n",
    "dat = dat[~np.isnan(dat['rt'])]\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the conditions\n",
    "for s in dat.stim.unique():\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get various ways to index the data\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "le = preprocessing.LabelEncoder()\n",
    "subj_idx = le.fit_transform(dat['subj'])\n",
    "cond_idx = le.fit_transform(dat['condition'])\n",
    "n_subj = len(dat.subj.unique())\n",
    "n_cond = len(dat.condition.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually best to look at log rt\n",
    "dat['log_rt'] = np.log(dat['rt'])\n",
    "\n",
    "# also add in some info about where the stimuli were presented\n",
    "dat['abs_x'] = np.round(np.abs(dat.loc_x), decimals=3)\n",
    "dat['abs_y'] = np.round(np.abs(dat.loc_y), decimals=3)\n",
    "dat['angle_rad'] = np.arctan2(dat.loc_y, dat.loc_x)\n",
    "dat['angle_deg'] = dat['angle_rad']*180/np.pi\n",
    "\n",
    "\n",
    "# show the columns we have to work with\n",
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how distribution of log_rts is mostly normal\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(dat['rt'].values, bins='auto');\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(dat['log_rt'].values, bins='auto');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore effects on reaction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show overall mean log_rt for each condition\n",
    "dat.groupby(['condition'])['log_rt'].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall mean:', dat.log_rt.mean())\n",
    "print('Overall std:', dat.log_rt.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical version of *t*-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hierarchical model\n",
    "with pm.Model() as hierarchical_model:\n",
    "    \n",
    "    # hyperprior for mu\n",
    "    mu_mu = pm.Normal('mu_mu', \n",
    "                        mu=dat.log_rt.mean(), \n",
    "                        sd=dat.log_rt.std()*2,\n",
    "                        shape=n_cond)\n",
    "    sigma_mu = pm.HalfCauchy('sigma_mu', 10, shape=n_cond)\n",
    "    \n",
    "    # prior on mu\n",
    "    mu = pm.Normal('mu', mu_mu, sigma_mu, \n",
    "                   shape=(n_subj, n_cond))\n",
    "    \n",
    "    # hyperprior for sd (Gelman suggests gamma prior)\n",
    "    sd_scale = pm.Gamma('sd_scale', .5, .5,\n",
    "                        shape=n_cond)\n",
    "    \n",
    "    # prior on sd\n",
    "    sd = pm.HalfCauchy('sd', sd_scale, \n",
    "                       shape=(n_subj, n_cond))\n",
    "    \n",
    "    # prior on df (fixed for all params)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # likelihood\n",
    "    log_rt_like = pm.StudentT('log_rt_like', \n",
    "                              mu=mu[subj_idx, cond_idx], \n",
    "                              sd=sd[subj_idx, cond_idx],\n",
    "                              nu=nu,\n",
    "                              observed=dat.log_rt)\n",
    "    \n",
    "    # save out some comparisons of interest\n",
    "    mudiff01 = pm.Deterministic('mu_diff_01', mu_mu[1]-mu_mu[0])\n",
    "    mudiff02 = pm.Deterministic('mu_diff_02', mu_mu[2]-mu_mu[0])\n",
    "    mudiff12 = pm.Deterministic('mu_diff_12', mu_mu[2]-mu_mu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine gamma prior on the half cauchy scale\n",
    "x = np.linspace(1, 20, 100)\n",
    "plt.plot(x, np.exp(pm.Gamma.dist(.5, .5).logp(x).eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hierarchical_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our traces\n",
    "#pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show comparisons of interest\n",
    "pm.plot_posterior(trace, varnames=['mu_diff_01', 'mu_diff_12', 'mu_diff_02'], \n",
    "                  ref_val=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical version of linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make a numerical condition variable\n",
    "dat['num_cond'] = 0\n",
    "dat.loc[dat['condition']=='=', 'num_cond'] = 1\n",
    "dat.loc[dat['condition']=='~', 'num_cond'] = 2\n",
    "dat.num_cond.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hierarchical model\n",
    "with pm.Model() as hierarchical_linear_model:\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_inter = pm.Normal('mu_inter', mu=dat.log_rt.mean(), sd=dat.log_rt.std()*10)\n",
    "    sigma_inter = pm.HalfCauchy('sigma_inter', 10)\n",
    "    \n",
    "    mu_slope = pm.Normal('mu_slope', mu=0., sd=dat.log_rt.std()*10)\n",
    "    sigma_slope = pm.HalfCauchy('sigma_slope', 10)\n",
    "\n",
    "    # Intercept for each subj, distributed around group mean\n",
    "    intercept = pm.Normal('intercept', mu=mu_inter, \n",
    "                          sd=sigma_inter, shape=n_subj)\n",
    "    \n",
    "    # slope for each subj, distributed around group mean\n",
    "    slope = pm.Normal('slope', mu=mu_slope, \n",
    "                      sd=sigma_slope, shape=n_subj)\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy('eps', 10)\n",
    "    \n",
    "    # define the means predicted from a linear function\n",
    "    log_rt_est = intercept[subj_idx] + slope[subj_idx] * dat.num_cond.values\n",
    "\n",
    "    # Data likelihood (could also replace with Student's t)\n",
    "    #log_rt_like = pm.Normal('log_rt_like', \n",
    "    #                        mu=log_rt_est, \n",
    "    #                        sd=eps, \n",
    "    #                        observed=dat.log_rt)\n",
    "    \n",
    "    # Data with Student's t likelihood\n",
    "    # prior on df (fixed for all params)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    log_rt_like = pm.StudentT('log_rt_like', mu=log_rt_est, sd=eps, nu=nu, observed=dat.log_rt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hierarchical_linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_linear_model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['mu_slope', 'mu_inter'], ref_val=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More-complicated linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hierarchical model\n",
    "with pm.Model() as hierarchical_linear_model_y:\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_inter = pm.Normal('mu_inter', mu=dat.log_rt.mean(), sd=dat.log_rt.std()*10)\n",
    "    sigma_inter = pm.HalfCauchy('sigma_inter', 10)\n",
    "    \n",
    "    mu_slope_cond = pm.Normal('mu_slope_cond', mu=0., sd=dat.log_rt.std()*10)\n",
    "    sigma_slope_cond = pm.HalfCauchy('sigma_slope_cond', 10)\n",
    "    mu_slope_y = pm.Normal('mu_slope_y', mu=0., sd=dat.log_rt.std()*10)\n",
    "    sigma_slope_y = pm.HalfCauchy('sigma_slope_y', 10)\n",
    "\n",
    "    # Intercept for each subj, distributed around group mean\n",
    "    intercept = pm.Normal('intercept', mu=mu_inter, sd=sigma_inter, shape=n_subj)\n",
    "    \n",
    "    # slope for each subj, distributed around group mean\n",
    "    slope_cond = pm.Normal('slope_cond', mu=mu_slope_cond, sd=sigma_slope_cond, shape=n_subj)\n",
    "    slope_y = pm.Normal('slope_y', mu=mu_slope_y, sd=sigma_slope_y, shape=n_subj)\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy('eps', 10)\n",
    "    \n",
    "    # define the means predicted from a linear function\n",
    "    log_rt_est = (intercept[subj_idx] + slope_y[subj_idx] * dat.abs_y.values + \n",
    "                  slope_cond[subj_idx] * dat.num_cond.values)\n",
    "\n",
    "    # Data likelihood (could also replace with Student's t)\n",
    "    #log_rt_like = pm.Normal('log_rt_like', mu=log_rt_est, sd=eps, observed=dat.log_rt)\n",
    "    \n",
    "    # Data with Student's t likelihood\n",
    "    # prior on df (fixed for all params)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    log_rt_like = pm.StudentT('log_rt_like', mu=log_rt_est, sd=eps, nu=nu, observed=dat.log_rt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hierarchical_linear_model_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_linear_model_y:\n",
    "    trace_y = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace_y, varnames=['mu_slope_cond', 'mu_slope_y'], ref_val=0.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a model comparison\n",
    "az.compare({'no_y': az.from_pymc3(trace), 'with_y': az.from_pymc3(trace_y)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate percent correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how about mean performance\n",
    "dat.groupby(['condition'])['correct'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.groupby(['condition'])['correct'].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hierarchical model\n",
    "with pm.Model() as perf_model:\n",
    "    \n",
    "    # prior on mu (one for each cond)\n",
    "    mu = pm.TruncatedNormal('mu',\n",
    "                            mu=dat.correct.mean(), \n",
    "                            sd=dat.correct.std()*2, \n",
    "                            lower=0.0, upper=1.0,\n",
    "                            shape=n_cond)\n",
    "    \n",
    "    # prior on sd (one for each cond)\n",
    "    sd = pm.HalfCauchy('sd', 0.1, shape=n_cond)\n",
    "    \n",
    "    # likelihood\n",
    "    prob = pm.Beta('beta', \n",
    "                   mu=mu,\n",
    "                   sd=sd,\n",
    "                   shape=(n_subj, n_cond))\n",
    "    \n",
    "    perf = pm.Binomial('perf', \n",
    "                       n=1,\n",
    "                       p=prob[subj_idx, cond_idx],\n",
    "                       observed=dat.correct)\n",
    "    \n",
    "    # save out some comparisons of interest\n",
    "    mudiff01 = pm.Deterministic('mu_diff_01', mu[1]-mu[0])\n",
    "    mudiff02 = pm.Deterministic('mu_diff_02', mu[2]-mu[0])\n",
    "    mudiff12 = pm.Deterministic('mu_diff_12', mu[2]-mu[1])\n",
    "    \n",
    "    sddiff01 = pm.Deterministic('sd_diff_01', sd[1]-sd[0])\n",
    "    sddiff02 = pm.Deterministic('sd_diff_02', sd[2]-sd[0])\n",
    "    sddiff12 = pm.Deterministic('sd_diff_12', sd[2]-sd[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore some priors\n",
    "x = np.linspace(0, 1, 100)\n",
    "#y = np.exp(pm.Beta.dist(mu=.5, sd=.15).logp(x).eval())\n",
    "y = np.exp(pm.HalfCauchy.dist(.1).logp(x).eval())\n",
    "#y = np.sqrt(x*(1-x))\n",
    "#y = np.exp(pm.TruncatedNormal.dist(mu=dat.correct.mean(), \n",
    "#                              sd=dat.correct.std()*2, \n",
    "#                              lower=0.0, upper=1.0).logp(x).eval())\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with perf_model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check our traces\n",
    "#pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show comparisons of interest\n",
    "pm.plot_posterior(trace, varnames=['mu_diff_01', 'mu_diff_12', 'mu_diff_02', \n",
    "                                   'sd_diff_01', 'sd_diff_12', 'sd_diff_02'], \n",
    "                  ref_val=0.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to do the comparison by hand\n",
    "probs = trace.get_values('beta')\n",
    "print(probs.shape)\n",
    "plt.hist(probs[:,0,0]-probs[:,0,1], bins='auto', alpha=.3);\n",
    "plt.hist(probs[:,0,1]-probs[:,0,2], bins='auto', alpha=.3);\n",
    "plt.axvline(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Dot Motion (RDM) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rdm_dataframe.csv', index_col='Unnamed: 0')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coherence.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in some columns of interest\n",
    "df['log_rt'] = np.log(df.rt)\n",
    "df['coh_diff'] = np.abs(df.left_coherence - df.right_coherence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['rt'], bins='auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data a bit\n",
    "df = df.loc[df.rt >= .25]\n",
    "plt.hist(df.log_rt, bins='auto');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['coh_diff'])['log_rt'].mean().plot()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
