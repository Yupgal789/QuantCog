{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 5: Bayesian Regression\n",
    "\n",
    "## Intro to Quantified Cognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson plan\n",
    "\n",
    "- Extension of BEST\n",
    "- Introduce Bayesian regression\n",
    "- Example with simulated data\n",
    "- Robust regression\n",
    "- Introduce hierarchical models\n",
    "- Hierarchical example\n",
    "- Real-world data analysis in teams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import pandas as pd               # efficient tables\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "from scipy import stats\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "import dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independent BEST\n",
    "\n",
    "Let's extend the example to independent samples!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data that may or may not be significantly different from each other\n",
    "A = dists.normal(mean=0.2, std=0.5).rvs(10)\n",
    "B = dists.normal(mean=0.4, std=1.0).rvs(12)\n",
    "\n",
    "# plot it\n",
    "plt.hist(A, bins='auto', alpha=0.3);\n",
    "plt.hist(B, bins='auto', alpha=0.3);\n",
    "\n",
    "# do a quick t-test\n",
    "stats.ttest_ind(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first get overall mean and std\n",
    "overall_mean = np.append(A, B).mean()\n",
    "overall_std = np.append(A, B).std()\n",
    "\n",
    "# define a model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    mu_A = pm.Normal('mu_A', overall_mean, overall_std*2.0)\n",
    "    sd_A = pm.HalfCauchy('sd_A', 5)\n",
    "    mu_B = pm.Normal('mu_B', overall_mean, overall_std*2.0)\n",
    "    sd_B = pm.HalfCauchy('sd_B', 5)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # build the model\n",
    "    #lam = data_std**-2.\n",
    "    data_A = pm.StudentT('data_A', mu=mu_A, sd=sd_A, nu=nu, observed=A)\n",
    "    data_B = pm.StudentT('data_B', mu=mu_B, sd=sd_B, nu=nu, observed=B)\n",
    "    \n",
    "    # set up some deterministic vars to keep\n",
    "    diff_of_means = pm.Deterministic('difference of means', mu_A - mu_B)\n",
    "    diff_of_stds = pm.Deterministic('difference of stds', sd_A - sd_B)\n",
    "    effect_size = pm.Deterministic('effect size',\n",
    "                                   diff_of_means / np.sqrt((sd_A**2 + sd_B**2) / 2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['mu_A', 'mu_B', 'sd_A', 'sd_B', 'df_minus_one', 'effect size']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['difference of means','difference of stds', 'effect size'],\n",
    "                  ref_val=0.0);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "One of the most common and flexible statistical approaches.\n",
    "\n",
    "Involves building a model that can predict the dependent data ($y$) based on different combinations of independent data ($x$):\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate some data with a linear trend\n",
    "nsamples = 100\n",
    "true_slope = 0.5\n",
    "true_intercept = 1.0\n",
    "true_sigma = 0.5\n",
    "\n",
    "# uniform sampling over x\n",
    "x = dists.uniform(0, 1).rvs(nsamples)\n",
    "\n",
    "# apply noise to linear model\n",
    "y_true = true_intercept + true_slope*x \n",
    "y = y_true + dists.normal(mean=0.0, std=true_sigma).rvs(nsamples)\n",
    "\n",
    "# set the data\n",
    "data = pd.DataFrame(dict(x=x, y=y))\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, y_true, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a standard linear model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    \n",
    "    # combine them into a linear function for the likelihood\n",
    "    likelihood = pm.Normal('y', mu=intercept + slope * x, \n",
    "                           sd=sigma, observed=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# sample the posterior\n",
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma'],\n",
    "                  ref_val=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dealing with outliers\n",
    "\n",
    "Sometimes data can be messy. You can either assume every observation affects the statistical inference similarly, or you can try and downplay the effect of potential outliers.\n",
    "\n",
    "This approach is also known as robust regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's add in some outliers!\n",
    "x_out = np.append(x, [.1, .15, .3])\n",
    "y_out = np.append(y, [3.54, 4.1, 3.2])\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# define a linear model with Gaussian noise\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    \n",
    "    # combine them into a linear function for the likelihood\n",
    "    likelihood = pm.Normal('y_out', mu=intercept + slope * x_out, \n",
    "                           sd=sigma, observed=y_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma'],\n",
    "                  ref_val=[true_intercept, 0.0, true_sigma]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check with the posterior predictives\n",
    "lm = lambda x, samples: samples['intercept'] + x*samples['slope']\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')\n",
    "\n",
    "pm.plot_posterior_predictive_glm(trace, eval=np.linspace(0, 1, 100), \n",
    "                                 lm=lm, samples=200, color=\"green\", alpha=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Can we fix it?\n",
    "# define a model\n",
    "with pm.Model() as model:\n",
    "    # set up the params/priors\n",
    "    intercept = pm.Normal('intercept', 0, 20)\n",
    "    slope = pm.Normal('slope', 0, 20)\n",
    "    sigma = pm.HalfCauchy('sigma', 10)\n",
    "    nu = pm.Exponential('df_minus_one', 1/29.) + 1.\n",
    "    \n",
    "    # combine them into a robust linear function for the likelihood\n",
    "    likelihood = pm.StudentT('y_out', mu=intercept + slope * x_out, \n",
    "                             sd=sigma, nu=nu, observed=y_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, varnames=['intercept', 'slope', 'sigma', 'df_minus_one'],\n",
    "                  ref_val=[true_intercept, 0.0, true_sigma, 0.0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check with the posterior predictives\n",
    "lm = lambda x, samples: samples['intercept'] + x*samples['slope']\n",
    "\n",
    "# plot the data\n",
    "plt.plot(x_out, y_out, 'o')\n",
    "plt.plot(x, y_true, '-')\n",
    "\n",
    "pm.plot_posterior_predictive_glm(trace, eval=np.linspace(0, 1, 100), \n",
    "                                 lm=lm, samples=200, color=\"green\", alpha=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Regression\n",
    "\n",
    "What should we do when we have multiple groups/subjects?\n",
    "\n",
    "Ideally we should share information across groups to inform the models fit to the individuals.\n",
    "\n",
    "This is called multi-level or hierarchical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data\n",
    "dat = pd.read_csv('data/flanker_dat.csv')\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually best to look at log rt\n",
    "dat['log_rt'] = np.log(dat['rt'])\n",
    "dat['subj'] = dat['subj']-101 #.astype('str')\n",
    "dat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.groupby(['condition'])['log_rt'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['num_cond'] = 0\n",
    "dat.loc[dat['condition']=='=', 'num_cond'] = 1\n",
    "dat.loc[dat['condition']=='~', 'num_cond'] = 2\n",
    "dat.num_cond.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get various ways to index the data\n",
    "subj_idx = dat.subj.values\n",
    "cond_idx = dat.condition.values\n",
    "\n",
    "n_subj = len(dat.subj.unique())\n",
    "n_cond = len(dat.condition.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors for group nodes\n",
    "    mu_a = pm.Normal('mu_a', mu=0., sd=100**2)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', 5)\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sd=100**2)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "\n",
    "    # Intercept for each county, distributed around group mean mu_a\n",
    "    # Above we just set mu and sd to a fixed value while here we\n",
    "    # plug in a common group distribution for all a and b (which are\n",
    "    # vectors of length n_subj).\n",
    "    a = pm.Normal('a', mu=mu_a, sd=sigma_a, shape=n_subj)\n",
    "    # slope for each subj, distributed around group mean mu_a\n",
    "    b = pm.Normal('b', mu=mu_b, sd=sigma_b, shape=n_subj)\n",
    "\n",
    "    # Model error\n",
    "    eps = pm.HalfCauchy('eps', 5)\n",
    "\n",
    "    log_rt_est = a[subj_idx] + b[subj_idx] * dat.num_cond.values\n",
    "\n",
    "    # Data likelihood\n",
    "    log_rt_like = pm.Normal('log_rt_like', mu=log_rt_est, sd=eps, observed=dat.log_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hierarchical_model:\n",
    "    trace = pm.sample(2000, cores=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
