{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lesson 12: Reinforcement Learning\n",
    "\n",
    "## Intro to Quantified Cognition\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/compmem/QuantCog/blob/2020_Spring/notebooks/12_Reinforcement_Learning.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lesson plan\n",
    "\n",
    "- Introduce Machine Learning\n",
    "- Fundamentals of Reinforcement Learning\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.app.compendium.com/uploads/user/e7c690e8-6ff9-102a-ac6d-e4aebca50425/03391c68-5c5e-493c-95a5-a92b492b9025/Image/5effec57771fb85b9fbaaafc87fbb27d/picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of RL\n",
    "\n",
    "- Reinforcement learning (RL) is can combine both Supervised and Unsupervised Learning.\n",
    "- The goal is for the *agent* to learn *actions* to take in an environment to maximize *reward*.\n",
    "\n",
    "### Policy\n",
    "\n",
    "Defines how the agent behaves in the environment. This is typically a mapping between perceived states of the world and possible actions. The policy is essentially the \"rules to live by\" for the agent.\n",
    "\n",
    "### Reward function\n",
    "\n",
    "The rewards (and punishments) define the goals of RL problem. The environment provides a reward signal to the agent at every time step. \n",
    "\n",
    "The policy can be adjusted to maximize rewards and the agent can influence the rewards by performing different actions in the environment to change the state or (where possible) alter the environment directly.\n",
    "\n",
    "### Value function\n",
    "\n",
    "This function estimates the predicted future reward for a given state or state--action pair. Learning this function is at the core of RL and is the information that guides the policy governing behavior in the environment.\n",
    "\n",
    "### Model of the environment (optional)\n",
    "\n",
    "Although not required for all RL algorithms, it is often useful for the agent to learn a model of the environment that estimates what agent *thinks* will happen when performing specific actions in a particular state. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Problem\n",
    "\n",
    "Most RL can be formalized as a Markov Decision Problem (MDP) that involves sensing the *states* and perfoming *actions* to achieve a *goal*. \n",
    "\n",
    "The key feature of an MDP is that the probability of ending up in some new state $s_{t+1}'$ given that you are in the current state $s_t$ and perform action $a_t$ only depends on the current state and not any previous states. That is to say, all of the history can be integrated into the current state $s_t.$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*QF9pmAOS5-Dn1B7RUH1zUA.png\" \n",
    "     alt=\"gridworld\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing *future* reward\n",
    "\n",
    "The goal of an agent is usually to maximize *future* reward, not just immediate reward. Thus an optimal policy can often have the agent forgo an immediate reward in order to gain more more rewards in the future:\n",
    "\n",
    "<img src=\"https://www.sciencenews.org/wp-content/uploads/2018/06/062918_BB_marshmallow-test_feat.jpg\" \n",
    "     alt=\"marshmallows\" width=\"500\"/>\n",
    "\n",
    "To keep this problem tracktable, instead of estimating out to infinity, the agent can estimate the *discounted* future reward:\n",
    "\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating value\n",
    "\n",
    "Estimating value is at the core of all RL algorithms. Value functions ($v$) are dependent on the policy ($\\pi$) for estimating future reward for a given state ($s$):\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4}... \\vert S_t=s)$$\n",
    "\n",
    "This estimates the discounted future value of a state given the current policy. To guide decisions for how to act, the agent must also estimate the value of taking a particular action in a given state, which can be learned via the temporal difference learning and the Bellman Equation:\n",
    "\n",
    "$$Q_\\pi(s_t, a_t) = Q_\\pi(s_t, a_t) + \\alpha (R_t + \\gamma max(Q_\\pi(s_{t+1}, a_{t+1})) - Q_\\pi(s_t, a_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs. Exploitation\n",
    "\n",
    "During learning, the agent must adopt a policy that balances exploration of the environment with exploitation of current knowledge. Usually this is accomplished by biasing the agent to make more random decisions at the start of learning and gradually shift to exploiting learned values later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successor Representation\n",
    "\n",
    "Hybrid of Model-based and Model-free RL developed by Peter Dayan (1993), where the agent learns a model of the world and, separately, the reward values at each state they've experienced. Specifically, the agent learns the expected discounted number of future visitations of all states given a current state and action:\n",
    "\n",
    "$$M = M + \\alpha(f_{i+1} + \\gamma Mf_{i+1} - Mf_i) f_i^T$$\n",
    "\n",
    "\n",
    "Thus, it is possible for and agent to make predictions of future states given specific state--actions and combine that information with known rewards/punishments to guide the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frisbee on a frozen lake problem\n",
    "\n",
    "\"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n",
    "\n",
    "https://gym.openai.com/envs/FrozenLake8x8-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *ONLY* if on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install more libraries\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules of power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load matplotlib inline mode\n",
    "%matplotlib inline\n",
    "\n",
    "# import some useful libraries\n",
    "import numpy as np                # numerical analysis linear algebra\n",
    "import matplotlib.pyplot as plt   # plotting\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "import gym\n",
    "from gym.envs.toy_text import frozen_lake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment\n",
    "size = 8\n",
    "p_frozen = 0.8\n",
    "slippery = False\n",
    "\n",
    "# generate a random map\n",
    "desc = frozen_lake.generate_random_map(size=size, p=p_frozen)\n",
    "env = frozen_lake.FrozenLakeEnv(desc=desc,\n",
    "                                is_slippery=slippery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SF\u001b[41mH\u001b[0mFFFFF\n",
      "FFHFFFFH\n",
      "FFFFHFFF\n",
      "FFFHFFHF\n",
      "HFFHFFHF\n",
      "FFFHHFFF\n",
      "FFFFFFHF\n",
      "HFFFFFFG\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 2 timesteps\n"
     ]
    }
   ],
   "source": [
    "### reset the environment and get the initial state\n",
    "observation = env.reset()\n",
    "display(print(env.render(mode='ansi')))\n",
    "\n",
    "# loop for a max number of iterations\n",
    "for t in range(100):\n",
    "    # pick an action at random\n",
    "    action = int(input('0=L, 1=D, 2=R, 3=U:'))\n",
    "    #action = env.action_space.sample()\n",
    "    \n",
    "    # take that action and observe the results\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # draw the new state to the screen\n",
    "    clear_output(wait=True)\n",
    "    display(print(env.render(mode='ansi')))\n",
    "    \n",
    "    # see if we're done (either by falling in a hole or reaching the goal)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "    time.sleep(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "gamma = .99\n",
    "alpha = .5\n",
    "rho = .25\n",
    "tau = 20.0\n",
    "p_rand = 0.0\n",
    "hole_penalty = -1.0\n",
    "off_board_penalty = -0.0\n",
    "\n",
    "# set up our agent\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "states = np.eye(n_states)\n",
    "rewards = np.zeros(n_states)\n",
    "M = np.zeros((n_actions, n_states, n_states))\n",
    "scores = []\n",
    "\n",
    "# define a policy\n",
    "def pick_action(f0, M, rewards, tau, p_rand=0.0):\n",
    "    # apply policy to pick action\n",
    "    if p_rand > 0.0 and np.random.rand() < p_rand:\n",
    "        # pick a random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        Q = np.dot(np.dot(M, f0), rewards)\n",
    "        #action = np.random.choice(np.where(Q==Q.max())[0])\n",
    "        #action = np.argmax(Q)\n",
    "        pQ = np.exp(Q*tau)/np.exp(Q*tau).sum()\n",
    "        action = np.argmax(np.random.rand() < np.cumsum(pQ))\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFHFFFFF\n",
      "FFHFFFFH\n",
      "FFFFHFFF\n",
      "FFFHFFHF\n",
      "HFFHFFHF\n",
      "FFFHHFFF\n",
      "FFFFFFHF\n",
      "HFFFFFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc539542d50>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in range(500):\n",
    "    # reset for new attempt at recovering the frisbee\n",
    "    observation = env.reset()\n",
    "    last_obs = observation\n",
    "    f0 = states[observation]\n",
    "    t0 = states[observation]\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # pick an action\n",
    "        action = pick_action(f0, M, rewards, tau, p_rand=p_rand)\n",
    "        \n",
    "        # observe new state\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # turn the new state into a vector representation\n",
    "        f1 = states[observation]\n",
    "\n",
    "        # learn via successor representation\n",
    "        # prediction from previous state\n",
    "        p0 = np.dot(M[action], f0)\n",
    "        \n",
    "        # observed outcome, plus discounted future prediction\n",
    "        # when following that policy\n",
    "        f1_action = pick_action(f1, M, rewards, tau, p_rand=p_rand)\n",
    "        o1 = (f1 + gamma*(np.dot(M[f1_action], f1)))\n",
    "        \n",
    "        # update the association for that action\n",
    "        M[action] += alpha * np.outer((o1 - p0), t0)\n",
    "\n",
    "        # update context (eligibility trace)\n",
    "        t1 = rho*t0 + (1-rho)*f1\n",
    "\n",
    "        # process the reward if any\n",
    "        if done and reward==0:\n",
    "            # get negative rewards for falling in a hole\n",
    "            reward = hole_penalty\n",
    "            \n",
    "        if last_obs == observation:\n",
    "            # action gave rise to no change in movement\n",
    "            reward = off_board_penalty\n",
    "\n",
    "        # update our representation of rewards/punishments at the observed state\n",
    "        rewards[observation] += alpha*(reward - rewards[observation])\n",
    "\n",
    "        # see if we're done\n",
    "        if done:\n",
    "            #print(\"Episode finished after {} timesteps with reward {}\".format(i+1, reward))\n",
    "            # save out our final reward/punishment\n",
    "            scores.append(reward)\n",
    "            break\n",
    "\n",
    "        # prepare for next iteration\n",
    "        f0 = f1\n",
    "        t0 = t1\n",
    "        last_obs = observation\n",
    "\n",
    "# render the final state\n",
    "env.render()\n",
    "\n",
    "# plot a moving average of scores\n",
    "N=50\n",
    "plt.plot(np.convolve(scores, np.ones((N,))/N, mode='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LEFT', 1.8456383296074665e-08) ('DOWN', 0.9999938053446692) ('RIGHT', 6.158255177501519e-06) ('UP', 1.7943769811239414e-08)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAC+CAYAAAAxxBclAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWRklEQVR4nO3de5BkZXnH8e/jslvcFkF2vMCiI4lFKaiAK15AY0ApLkb9IxdMNKUx2TKFBkzUwjKJlypTZWKQpGI0BFEqCpaiqEGjYonlDdHZC8qyooAg6wI7gMguiMDy5I8+U/Quy/bt7dOnz3w/VV0z3dP962d6nj37zJnT543MRJIkSWqTx0y6AEmSJKk0h1xJkiS1jkOuJEmSWschV5IkSa3jkCtJkqTWcciVJElS69Q+5EbESRFxbURcFxFnjZBzfkRsiYirR6znkIi4PCI2RsSGiDhjyJw9I+IHEXFVlfOeEetaEhHrIuLSETJujIgfR8T6iJgbIWf/iLg4In5SvU4vGDLnsKqWhcvdEXHmsHXVrUTvNq1vq6xivVuib6ucxvTutPcttLN3m7jNrXLs3UKcF/rOc154NJlZ2wVYAlwPHAosA64CnjFk1ouBo4GrR6zpScDR1efLgZ8OUxMQwL7V50uBK4Hnj1DX3wIXApeOkHEjsKLAz+0C4C+rz5cB+xfqhVuBp4y770pcSvVu0/q2enyx3i3Rt1VOI3t32vq2q+bW9W4Tt7lVjr1b4FKqb6sse7d3RiP7tqsXhurduvfkHgNcl5k3ZOb9wKeAVw4TlJnfAu4ctaDMvCUz11afbwU2AgcPkZOZua26urS6DLXSRkSsBE4Fzhvm8SVFxH50NhAfBcjM+zPzrgLRJwDXZ+ZNBbLqUKR3m9a31eOL9G6T+hbG1rvT1rfQ0t5t6zYX7N2K80IfmtS7TZwX6h5yDwZu7rq+iSH/Yx6HiJgFjqLzW9Uwj18SEeuBLcBlmTlUDnAO8HbgoSEfvyCBr0XEmohYPWTGocA88LHqzyHnRcQ+I9YFcBpwUYGcujS2d0ft2yqjRO+W6ltobu9OW99Ci3u3gdtcsHdLaWzfQit7t6l9CyP0bt1DbuzitkasKxwR+wKfBc7MzLuHycjM7Zl5JLASOCYijhiijpcDWzJzzTA17OTYzDwaOBk4PSJePETGHnT+zPPhzDwKuAcY+tgogIhYBrwC+MwoOTVrZO+W6FsYvXcL9y00sHentG+hxb3bwG0u2LulNLJvobW927i+hdF7t+4hdxNwSNf1lcDmmmt4hIhYSqdhP5mZnxs1r9o9/03gpCEefizwioi4kc6fZ46PiE8MWcfm6uMW4BI6f/4Z1CZgU9dvmRfTaeJRnAyszczbRsypU+N6t3Tfwki9W6xvqzqa2LvT2LewCHq3KdvcqhZ7t4zG9S20t3cb2rcwYu/WPeT+EHhaRDy1ms5PA75Ycw07iIigc/zIxsw8e4ScmYjYv/p8L+ClwE8GzcnMd2TmysycpfP6fCMzXzNEPftExPKFz4ETgYHfWZqZtwI3R8Rh1U0nANcMmrOTVzNdfzaDhvVuqb6tskbu3VJ9W9XQ1N6dxr6FlvZu07a5VR32bjmN6ltob+82uG9h1N7NEd/1NugFOIXOOxKvB945Qs5FwC3AA3R+e3jDkDnH0fkTyI+A9dXllCFyngWsq3KuBv6xwGv1EoZ8tySdY2Ouqi4bRnytjwTmqu/t88ABI2TtDdwBPLbu3ivw8xi5d5vWt1VW0d4dpW+rxzeud6e5b6v6W9e7TdvmVo+3dwteSvRtlWPv7v6xjevbKmvk3o0qSJIkSWoNVzyTJElS6zjkSpIkqXUcciVJktQ6DrmSJElqnYkNuSOsqGHOhLKaljMJ/lymL6dklr3bvJySWeY0U9Neh7bmlMxqSs4k9+SW+qGYU19W03ImwZ/L9OWUzLJ3m5dTMsucZmra69DWnJJZjcjxcAVJkiS1zljOk7tixYE5O3vIbu8zP38HMzMH7j5o/qqezzW/DWb23f191v2iZwzJrhfKHlTTckpm1ZnzEJCZpV6CvkRE9vqtb7H/XKYxp2RWn717e2bOFHi6vq04IHL2oN3fZ/5XMHNAj6C9lvd8rvn5+5mZWbbb+6xbs7VnzrT+fNuaM4ltLsCKFStydnZ2t/eZn59nZqbHP6k71/R8rvmtMNOjxdf9vGfMVP58685qSu/uUaCGR5idPYS5ua+PHvSRMv9P7PPXRWIAWFIoZ3uhnLa6bwLP+Rhgzwk8r9rlXrip7uecPQjmPl0g6PBVBUJgn7i8SI7qM4ltLsDs7Cxzc3OjB11YZszb58+KxADOC3XZXe96uIIkSZJaxyFXkiRJreOQK0mSpNZxyJUkSVLr9DXkRsRJEXFtRFwXEWeNuyhJkiRpFD2H3IhYAnwIOBl4BvDqiHjGuAuTJEmShtXPntxjgOsy84bMvB/4FPDK8ZYlSZIkDa+fIfdg4Oau65uq23YQEasjYi4i5ubn7yhVnzRW3X1bflkUaXx22Ob+atLVSP3bcV6Yn3Q5arF+htxdnWH5EfNAZp6bmasyc1XPlcykhuju29qX+pFGsMM2t9dKZlKD7Dgv1Lo4oBaZfobcTUD3Gr0rgc3jKUeSJEkaXT9D7g+Bp0XEUyNiGXAa8MXxliVJkiQNb49ed8jMByPiTcBX6SzFfH5mbhh7ZZIkSdKQeg65AJn5ZeDLY65FkiRJKsIVzyRJktQ6DrmSJElqHYdcSZIktU5fx+QO7mfAqaPHfHT0iNL2LJRzT6EclfNk4L2TLqLLGyddgKbGg9fAliNGz3n80y8fPaSwpYVyHiiUo9J+Cpw4ekwD3zXkvDB57smVJElS6zjkSpIkqXUcciVJktQ6DrmSJElqHYdcSZIktU7PITcizo+ILRFxdR0FSZIkSaPqZ0/ux4GTxlyHJEmSVEzPITczvwXcWUMtkiRJUhEekytJkqTWKTbkRsTqiJiLiLn5+QdLxUpj1d23WyddjDSA7t69Y9LFSAPYcV5wLTqNT7EhNzPPzcxVmblqZmZMqwVLhXX37fJJFyMNoLt3D5x0MdIAdpwXSi3cLD2ShytIkiSpdfo5hdhFwBXAYRGxKSLeMP6yJEmSpOH1PK4gM19dRyGSJElSKR6uIEmSpNZxyJUkSVLrOORKkiSpdcZzrq+f3gsv+8HIMb+cK1ALcFyZGACuLZRzT6EclfML4I0Fcv6zQIY0iLuALxTIef7GAiHAYWViALitUM5dhXJU2Mat8NzLRo65u9C8cHyZGACuKZTjvDA89+RKkiSpdRxyJUmS1DoOuZIkSWodh1xJkiS1jkOuJEmSWschV5IkSa3Tc8iNiEMi4vKI2BgRGyLijDoKkyRJkobVz3lyHwT+LjPXRsRyYE1EXJaZpU4BJ0mSJBXVc09uZt6SmWurz7cCG4GDx12YJEmSNKyBVjyLiFngKODKXXxtNbAa4Ml7FqhMqkF338aEa5EG0d27j5twLdIgdpgXlk24GLVa3288i4h9gc8CZ2bm3Tt/PTPPzcxVmblqZmnJEqXx6e5bh1xNk+7eXT7pYqQB7DAvDLSrTRpMX0NuRCylM+B+MjM/N96SJEmSpNH0c3aFAD4KbMzMs8dfkiRJkjSafvbkHgu8Fjg+ItZXl1PGXJckSZI0tJ5Hw2Tmd/A9OZIkSZoirngmSZKk1nHIlSRJUus45EqSJKl1xnKGutu2wge+PnpOqXM/7lcoB2C+YJbaafukC9CiczvwsQI5mwtkADxUKAfgroJZap7N98K75kbPKTUvlDzN/60FszQc9+RKkiSpdRxyJUmS1DoOuZIkSWodh1xJkiS1jkOuJEmSWqfnkBsRe0bEDyLiqojYEBHvqaMwSZIkaVj9nELst8DxmbktIpYC34mI/8vM74+5NkmSJGkoPYfczExgW3V1aXXJcRYlSZIkjaKvY3IjYklErAe2AJdl5pW7uM/qiJiLiLltj4yQGqm7b/3NTdOku3cfmHQx0gC6e/feSRejVutryM3M7Zl5JLASOCYijtjFfc7NzFWZuWrf0lVKY9LdtzHpYqQBdPduyVWapHHr7t29J12MWm2gsytk5l3AN4GTxlKNJEmSVEA/Z1eYiYj9q8/3Al4K/GTchUmSJEnD6ufsCk8CLoiIJXSG4k9n5qXjLUuSJEkaXj9nV/gRcFQNtUiSJElFuOKZJEmSWschV5IkSa3jkCtJkqTW6eeNZwPbDLyrQM7zCmQAeLJpSW32G+CqAjlPLJABMF8oR+13G/CBAjlHF8gAuKdQjprBPbmSJElqHYdcSZIktY5DriRJklrHIVeSJEmt45ArSZKk1ul7yI2IJRGxLiJc0leSJEmNNsie3DOAjeMqRJIkSSqlryE3IlYCpwLnjbccSZIkaXT9LgZxDvB2YPmj3SEiVgOrAWL0uqRa2LeaVvauppW9q7r03JMbES8HtmTmmt3dLzPPzcxVmbnKptW0sG81rexdTSt7V3Xp53CFY4FXRMSNwKeA4yPiE2OtSpIkSRpBzyE3M9+RmSszcxY4DfhGZr5m7JVJkiRJQ/I8uZIkSWqdft94BkBmfhP45lgqkSRJkgpxT64kSZJaxyFXkiRJreOQK0mSpNYZ6JjcfgWwZ4GcmwpkANxTKEfqx/ZJFyAN6c5COVsL5aj9AlhaIOfGAhkADxTKUTO4J1eSJEmt45ArSZKk1nHIlSRJUus45EqSJKl1HHIlSZLUOn2dXSEibqTzhtntwIOZuWqcRUmSJEmjGOQUYr+fmbePrRJJkiSpEA9XkCRJUuv0O+Qm8LWIWBMRq3d1h4hYHRFzETGX5eqTxsq+1bSydzWtunv3oUkXo1aLzN6bx4g4KDM3R8TjgcuAN2fmtx7t/ntE5GMLFLd/gQwou+KZK/nU4z5ge2bU+ZxLIrLESn3nFMgAOLNQjup1L6yp+30LpXr3uQUyANYWygFXEKzLJLa50JkX9i2Qs7xABpRd8cx5oR67692+9uRm5ubq4xbgEuCYYtVJkiRJhfUcciNin4hYvvA5cCJw9bgLkyRJkobVz9kVngBcEhEL978wM78y1qokSZKkEfQccjPzBuDZNdQiSZIkFeEpxCRJktQ6DrmSJElqHYdcSZIktc4gy/r2LSlzrrk7CmQALCmUUzLLcz9Kapr7C+W4fVO/EiixIMSvC2QALC2UA84LTeCeXEmSJLWOQ64kSZJaxyFXkiRJreOQK0mSpNZxyJUkSVLr9DXkRsT+EXFxRPwkIjZGxAvGXZgkSZI0rH5PIfZvwFcy8w8jYhmw9xhrkiRJkkbSc8iNiP2AFwOvA8jM+yl3OkVJkiSpuH4OVzgUmAc+FhHrIuK8iNhnzHVJkiRJQ+tnyN0DOBr4cGYeBdwDnLXznSJidUTMRcRcFi5SGhf7VtPK3tW0sndVl8jcfYtFxBOB72fmbHX9RcBZmXnqoz1mSUSW2NVb6tQPJZf1LbFcMbhMXy/3Adszo87nXBKRexbIOadABsCZhXJUr3thTWauqvM5S/XuswtkAFxVKEf1mcQ2F8rNC6WUXNbXeaEeu+vdnnNkZt4K3BwRh1U3nQBcU648SZIkqax+z67wZuCT1ZkVbgBeP76SJEmSpNH0NeRm5nqg1j+/SZIkScNyxTNJkiS1jkOuJEmSWschV5IkSa3jkCtJkqTW6ffsCgNr0nndHpp0AVpUSvXb2YVyoFn/HqHca9S07ws8vzHAMyddgAZ29QSfu8S/41Lnwy91bls1g3tyJUmS1DoOuZIkSWodh1xJkiS1jkOuJEmSWschV5IkSa3Tc8iNiMMiYn3X5e6I8A3EkiRJaqyepxDLzGuBIwEiYgnwS+CSMdclSZIkDW3QwxVOAK7PzJvGUYwkSZJUwqCLQZwGXLSrL0TEamA1QIxYlFQX+1bTyt7VtLJ3VZfIzP7uGLEM2Awcnpm37e6+SyJyzwLFlVrBpImauFJTk9wHbM+sdftXqm9LrlRWStP6reUrnq3JzFV1Pmep3n12gQxwlclpdDWwreZtLjgv9KOJ27km2d28MMjhCicDa3sNuJIkSdKkDTLkvppHOVRBkiRJapK+htyI2Bt4GfC58ZYjSZIkja6vN55l5r3AgWOuRZIkSSrCFc8kSZLUOg65kiRJah2HXEmSJLVO3+fJHSg0Yh7otSraCuD2Ak9nTn1ZdeY8JTNnCjxX32ru25JZ5tSXZe82L6dk1mLOqb1vYdH3btNySmY1onfHMuT2IyLmSpww3Zzpq6nk91Y3fy7Tl9PUmurWtNegzT/ftuZMStNeh7bmNLGmUXM8XEGSJEmt45ArSZKk1pnkkHuuObXklMxqWs4k+HOZvpySWfZu83JKZpnTTE17HdqaUzKrETkTOya3jSJiW2buu9Nt7wb+CpjvuvklwJHAF4CfV7fdDlwO/FF1/ZnAj6vPz8/Mfx9P1VpsImI7nd5aCjwIXACck5kPVV8/Djgb2K96yNmZeW5E7A9cD6zIzIyIFwDfAw7JzE0R8Vg6/bwCOJ/OKomHZuZvI2IFMJeZs7V9o2qdrt7dg06vvTYz74qIWeDSzDyiut8xwD8DBwNbgVuAszLzx9U2eVtmfqAr90bgecBXq5ueCGzn4e32MZl5/1i/OS16O/dxddu7gW3AEcDvAb8GHgJOz8wr6q9yuvS14plG9sHuDSpARAB8OzNfvtN931d9fVtmHllTfVpcfrPQWxHxeOBC4LHAuyLiidX1V2Xm2mo4/WpE/DIzvxQRtwJPB64BXgisqz5+Gng+cGVmPlT193bgL4AP1/vtqcW6e/cC4HSqbeaCiHgCnX7808z8XnXbccDv8PCOg13Z3pX9bnYahKUGeFtmXhwRJwL/BTxr0gU1ncfkSotYZm4BVgNvis5kejrw8cxcW339duDtwFnVQ75LZ6il+vjBna5/ryv+HOAtEeEv0xqHK+jsqd3Zm4ALFgZcgMz8TmZ+vrbKpPH6FvC7ky5iGjjk1uMtEbG+ulzedfuLum5/58Sq06KWmTfQ2RY8HjgcWLPTXeaq26EzxC4MtYcCnwEWTu/yQjpD8IJfAN8BXlu+ai1mEbEEOAH44i6+fDiwtkdE9zZ5PXBQ6RqlMfoDdv9XCVXcw1KPRxyuUNnV4QrSJETXx10dqL9w23eBsyLiqcCNmXlfdOwLPAf4wU6P+yc6g8iXxlCzFp+9qqF0ls4vY5f1ekBEXEnn+PKvZeYZ1c0f3MUxudKkPdqbpBZu/5eI+Hs6x4q/oZ6Sppt7cqVFLiIOpXP87BZgAw/vmV3wHDrH4JKZPwMOoLMnYeFND2uA1wM/z8xt3Q/MzOuA9cAfj6t+LSoLx+Q+BVhG5/CanW0Ajl64kpnPA/6BznHnUpPdQWf72u1xPLzi19sy88jMfFlmXl1vadPJIVdaxCJiBvgI8B/ZOdXKh4DXRcTCG3AOBN5P553qC64AzuDhIfcK4Ex2PB632/uAt5avXotVZv4a+BvgrRGxdKcvL/TwC7tu27u24qQhVTsJbomIEwAi4nHASXQO+9IQPFyhrL0jYlPX9bOrj2+JiNd03f6qGmuSdrbwJ9+FU4j9D1WvZuYtVa/+d0Qsp3P4wjmZ+b9dj/8ucAqdY3WhM+QeyqMMuZm5ISLW0rV3TRpVZq6LiKuA04Bvd91+a0T8CfD+iDiYzl8obgfeO5lKpYH8OfChiPjX6vp7MvP66ow1GpDnyZUkSVLreLiCJEmSWschV5IkSa3jkCtJkqTWcciVJElS6zjkSpIkqXUcciVJktQ6DrmSJElqnf8HOvg9KK9n4j8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see predicted outcomes for different actions at a particular state\n",
    "state = 0\n",
    "fig,ax = plt.subplots(1, 4, figsize=(12,5), sharex=True, sharey=True)\n",
    "acts = ['LEFT', 'DOWN', 'RIGHT', 'UP']\n",
    "\n",
    "# get the min and max values for plot normalization\n",
    "pmin = 0.0\n",
    "pmax = 0.0\n",
    "for i in range(n_actions):\n",
    "    pred = np.dot(M, states[state])[i]\n",
    "    pmin = min(pred.min(), pmin)\n",
    "    pmax = max(pred.max(), pmax)\n",
    "\n",
    "# do the plot\n",
    "for i in range(n_actions):\n",
    "    pred = np.dot(M, states[state])[i]\n",
    "    ax[i].matshow((pred.reshape((size, size))), vmin=pmin, vmax=pmax, cmap='hot')\n",
    "    ax[i].set_xlabel(acts[i])\n",
    "\n",
    "# print out model-based predictions for state-actions\n",
    "Q = np.dot(np.dot(M, states[state]), rewards)\n",
    "pQ = np.exp(Q*tau)/np.exp(Q*tau).sum()\n",
    "print(*zip(acts, pQ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fc5391c7990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see what state rewards it's learned\n",
    "plt.matshow(rewards.reshape((size, size)), cmap='bwr_r', vmin=-1, vmax=1.0)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFHFFFFF\n",
      "FFHFFFFH\n",
      "FFFFHFFF\n",
      "FFFHFFHF\n",
      "HFFHFFHF\n",
      "FFFHHFFF\n",
      "FFFFFFHF\n",
      "HFFFFFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 14 timesteps\n"
     ]
    }
   ],
   "source": [
    "# let's see the trained model in action!\n",
    "\n",
    "# reset the to starting point\n",
    "observation = env.reset()\n",
    "f0 = states[observation]\n",
    "for t in range(100):\n",
    "    # pick an action following our policy\n",
    "    action = pick_action(f0, M, rewards, tau, p_rand=p_rand)\n",
    "    \n",
    "    # observe the next state\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    f0 = states[observation]\n",
    "    \n",
    "    # print it to the screen\n",
    "    clear_output(wait=True)\n",
    "    display(print(env.render(mode='ansi')))\n",
    "    \n",
    "    # see if we're done\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "    time.sleep(.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_desc = desc[:]\n",
    "drow = list(new_desc[-1])\n",
    "drow[-1] = 'H'\n",
    "new_desc[-1] = ''.join(drow)\n",
    "drow = list(new_desc[3])\n",
    "drow[-1] = 'G'\n",
    "new_desc[3] = ''.join(drow)\n",
    "new_desc\n",
    "env = frozen_lake.FrozenLakeEnv(desc=new_desc,\n",
    "                                is_slippery=slippery)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booyah!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
